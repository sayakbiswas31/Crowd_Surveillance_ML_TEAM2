# -*- coding: utf-8 -*-
"""gender classification body.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nR-50F4dmhhjhEqjxHx7cfhqumfWRiyB
"""

!pip install torch torchvision albumentations opencv-python numpy pillow matplotlib rembg

import os
import shutil
import random

def split_dataset(input_dir, output_dir, train_ratio=0.8):
    categories = ['male', 'female']

    for category in categories:
        category_dir = os.path.join(input_dir, category)
        videos = [f for f in os.listdir(category_dir) if f.endswith('.mp4')]

        random.shuffle(videos)
        train_count = int(len(videos) * train_ratio)

        train_videos = videos[:train_count]
        val_videos = videos[train_count:]

        train_output_dir = os.path.join(output_dir, 'train', category)
        val_output_dir = os.path.join(output_dir, 'val', category)

        os.makedirs(train_output_dir, exist_ok=True)
        os.makedirs(val_output_dir, exist_ok=True)

        # Move files
        for video in train_videos:
            shutil.copy(os.path.join(category_dir, video), os.path.join(train_output_dir, video))

        for video in val_videos:
            shutil.copy(os.path.join(category_dir, video), os.path.join(val_output_dir, video))

        print(f"{category}: {len(train_videos)} train, {len(val_videos)} val")

# Example usage
split_dataset('/content/drive/MyDrive/dataset', '/content/drive/MyDrive/dataset_split', train_ratio=0.8)

import cv2
import os

def extract_frames_from_videos(video_dir, output_dir, frame_skip=10):
    os.makedirs(output_dir, exist_ok=True)
    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]

    for video_file in video_files:
        video_path = os.path.join(video_dir, video_file)
        cap = cv2.VideoCapture(video_path)
        count = 0
        frame_id = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            if count % frame_skip == 0:
                frame_filename = os.path.join(output_dir, f"{os.path.splitext(video_file)[0]}_frame_{frame_id}.jpg")
                cv2.imwrite(frame_filename, frame)
                frame_id += 1

            count += 1

        cap.release()

# Extract frames for each split and class
for split in ['train', 'val']:
    for category in ['male', 'female']:
        input_dir = f'/content/drive/MyDrive/dataset_split/{split}/{category}'
        output_dir = f'/content/frames/{split}/{category}'
        extract_frames_from_videos(input_dir, output_dir)

import torch
from PIL import Image
import os

# Load YOLOv5 pretrained model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

def crop_person_from_images(input_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    image_files = [f for f in os.listdir(input_dir) if f.endswith('.jpg')]

    for image_file in image_files:
        img_path = os.path.join(input_dir, image_file)
        results = model(img_path)
        detections = results.pandas().xyxy[0]
        persons = detections[detections['name'] == 'person']

        img = Image.open(img_path)

        for index, row in persons.iterrows():
            x1, y1, x2, y2 = row[['xmin', 'ymin', 'xmax', 'ymax']]
            cropped = img.crop((x1, y1, x2, y2))
            cropped.save(os.path.join(output_dir, f"{os.path.splitext(image_file)[0]}_crop_{index}.jpg"))

# Run cropping for each split and class
for split in ['train', 'val']:
    for category in ['male', 'female']:
        input_dir = f'/content/frames/{split}/{category}'
        output_dir = f'/content/cropped/{split}/{category}'
        crop_person_from_images(input_dir, output_dir)

from torchvision import datasets, transforms, models
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

train_dataset = datasets.ImageFolder('/content/cropped/train', transform=transform)
val_dataset = datasets.ImageFolder('/content/cropped/val', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Model
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(10):
    model.train()
    running_loss = 0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")

    # Validation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    print(f"Validation Accuracy: {100 * correct / total:.2f}%")

torch.save(model.state_dict(), '/content/model.pth')

import torch
from torchvision import transforms, models
import torch.nn as nn

# Load model
model = models.resnet18(pretrained=False)
model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes
model.load_state_dict(torch.load('/content/model.pth'))
model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Define transforms (same as during training)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

class_names = ['female', 'male']

yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

import torch
from torchvision import transforms, models
from google.colab.patches import cv2_imshow
import torch.nn as nn
import cv2
from PIL import Image
import numpy as np

# Load CNN model
model = models.resnet18(pretrained=False)
model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes: female, male
model.load_state_dict(torch.load('/content/model.pth', map_location='cpu'))
model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Class names
class_names = ['female', 'male']

# Transform (same as training)
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])
])

# Load YOLOv5
yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Open video
input_video = '/content/drive/MyDrive/crowd.mp4'
cap = cv2.VideoCapture(input_video)

frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Save frame temporarily
    temp_img_path = '/content/temp_frame.jpg'
    cv2.imwrite(temp_img_path, frame)

    # Detect person with YOLO
    results = yolo_model(temp_img_path)
    detections = results.pandas().xyxy[0]
    persons = detections[detections['name'] == 'person']

    frame_labels = []  # store predictions for this frame

    for index, row in persons.iterrows():
        x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        person_crop = frame[y1:y2, x1:x2]

        if person_crop.size == 0:
            continue

        # Convert to PIL & transform
        pil_img = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))
        input_tensor = transform(pil_img).unsqueeze(0).to(device)

        # Predict
        with torch.no_grad():
            output = model(input_tensor)
            _, pred = torch.max(output, 1)
            label = class_names[pred.item()]
            frame_labels.append(label)

        # Draw box + label on frame
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
        cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)

    # Print labels for this frame
    print(f"Frame {frame_count}: {frame_labels}")

    # Display the frame
    cv2_imshow(frame)

cap.release()