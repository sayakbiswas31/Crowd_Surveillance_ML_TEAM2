# -*- coding: utf-8 -*-
"""gender calssification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ny4w9KUfhUNzlH0APQ4UIWhSfOjHT4o_
"""

!pip install -q insightface
!pip install -q onnxruntime

from insightface.app import FaceAnalysis

# Initialize RetinaFace detector
app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
app.prepare(ctx_id=0)

!wget -q https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt -O /content/deploy.prototxt
!wget -q https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel -O /content/res10_300x300_ssd_iter_140000.caffemodel

!pip install -q insightface
import cv2
import torch
import numpy as np
from PIL import Image
from transformers import ViTForImageClassification, ViTImageProcessor
from torchvision import transforms
from google.colab.patches import cv2_imshow
from insightface.app import FaceAnalysis
from torchvision.ops import box_iou

# Initialize the models
model_name = "rizvandwiki/gender-classification"
model = ViTForImageClassification.from_pretrained(model_name)
processor = ViTImageProcessor.from_pretrained(model_name)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),
])

face_net = cv2.dnn.readNetFromCaffe(
    "/content/deploy.prototxt",
    "/content/res10_300x300_ssd_iter_140000.caffemodel"
)

app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
app.prepare(ctx_id=0)

# Load the video file
video_path = "/content/drive/MyDrive/crowd.mp4"
cap = cv2.VideoCapture(video_path)
frame_count = 0
max_frames = 10  # Number of frames to process

# Function to detect faces using RetinaFace
def get_boxes_from_retina(frame):
    faces = app.get(frame)
    boxes = []
    for face in faces:
        x1, y1, x2, y2 = face.bbox.astype(int)
        boxes.append([x1, y1, x2, y2])
    return boxes

# Function to detect faces using OpenCV DNN
def get_boxes_from_opencv(frame):
    (h, w) = frame.shape[:2]
    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0))
    face_net.setInput(blob)
    detections = face_net.forward()
    boxes = []
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        if confidence > 0.6:
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            boxes.append(box.astype(int).tolist())
    return boxes

# Merge boxes from both detectors using IOU
def merge_boxes(boxes1, boxes2, iou_threshold=0.3):
    b1 = torch.tensor(boxes1)
    b2 = torch.tensor(boxes2)
    if len(b1) == 0:
        return boxes2
    if len(b2) == 0:
        return boxes1

    iou = box_iou(b1, b2)
    keep_b2 = ~iou.max(0).values.gt(iou_threshold)
    combined = boxes1 + [boxes2[i] for i, keep in enumerate(keep_b2) if keep]
    return combined

# Main processing loop
while cap.isOpened() and frame_count < max_frames:
    ret, frame = cap.read()
    if not ret:
        break

    (h, w) = frame.shape[:2]

    male_count = 0
    female_count = 0

    boxes_opencv = get_boxes_from_opencv(frame)
    boxes_retina = get_boxes_from_retina(frame)

    all_boxes = merge_boxes(boxes_opencv, boxes_retina)

    for (x1, y1, x2, y2) in all_boxes:
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)

        face = frame[y1:y2, x1:x2]
        if face.size == 0:
            continue

        face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
        face_tensor = transform(face_pil).unsqueeze(0)

        with torch.no_grad():
            outputs = model(face_tensor)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            pred = torch.argmax(probs).item()
            label = model.config.id2label[pred]

        if label == "male":
            male_count += 1
        else:
            female_count += 1

        color = (255, 0, 0) if label == "Male" else (0, 0, 255)
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

    cv2.putText(frame, f'Male: {male_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
    cv2.putText(frame, f'Female: {female_count}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    cv2_imshow(frame)
    print(f"Frame {frame_count} - Males: {male_count}, Females: {female_count}")  # Print the count for the current frame
    frame_count += 1

cap.release()

torch.save(model.state_dict(), '/content/model.pth')